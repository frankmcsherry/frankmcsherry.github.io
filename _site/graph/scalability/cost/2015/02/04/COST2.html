<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width initial-scale=1">

  <title>Bigger data; same laptop</title>
  <meta name="description" content="This post follows up the previous post, “Scalability! But at what COST?” which got a great response. The short version of the previous post is that for the g...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://www.frankmcsherry.org/graph/scalability/cost/2015/02/04/COST2.html">
  <link rel="alternate" type="application/atom+xml" title="Frank McSherry" href="http://www.frankmcsherry.org/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Frank McSherry</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">Some Background</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Bigger data; same laptop</h1>
    <p class="post-meta">Feb 4, 2015</p>
  </header>

  <article class="post-content">
    <p>This post follows up the previous post, “Scalability! But at what COST?” which got a great response. The short version of the previous post is that for the graph datasets and computations the scalable systems research community is currently looking at, a laptop outperforms the scalable systems.</p>

<p>There were several flavors of response to the post (many of which were very supportive!), but one that I’d like to address in this post is</p>

<blockquote>
  <p>One billion edges is just a few gigabytes; it’s hardly “big data” …</p>
</blockquote>

<p>My personal opinion is that if your system doesn’t perform well on one billion things, you don’t yet have the credibility to claim it’s going to work much better on a trillion things. But let’s see where this goes …</p>

<p>The only reported data I know of for graph processing at the “one trillion things” scale is <a href="https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920">Facebook’s processing of a one trillion edge graph</a>, where they got the per-iteration pagerank time down to under four minutes on 200 commodity machines (number of cores unknown to me). That’s really quite good (and, much better than Giraph performed in GraphX’s evaluation).</p>

<p>I don’t have a trillion edge graph, unfortunately, but thanks to the nice people at <a href="http://webdatacommons.org">Web Data Commons</a>, I was able to download a <a href="http://webdatacommons.org/hyperlinkgraph/index.html">128B edge graph</a>, based on the hyperlink structure from <a href="http://commoncrawl.org">Common Crawl</a>. Using the same algorithms from the previous post yields:</p>

<center>
<table border="1" style="width:75%">
<tr> <th>System</th> <th>cores</th> <th>twitter_rv</th> <th>uk_2007_05</th><th>common crawl</th></tr>
<tr> <td>Single thread</td> <td align="right">1</td> <td align="right">242s</td><td align="right">256s</td><td align="right">23653s</td></tr>
<caption>Twenty pagerank iterations</caption>
</table>
</center>

<p></p>

<center>
<table border="1" style="width:75%">
<tr> <th>System</th> <th>cores</th> <th>twitter_rv</th> <th>uk_2007_05</th><th>common crawl</th></tr>
<tr> <td>Single thread</td> <td align="right">1</td> <td align="right">15s</td><td align="right">30s</td><td align="right">1700s</td></tr>
<caption>Union find (graph connectivity)</caption>
</table>
</center>

<p></p>
<p>Importantly, these algorithms are <em>exactly</em> the same as in the previous post. Nothing has been changed, other than (a few sections down) a different on-disk representation of the graph so that it fit on my SSD. The edges per second are mostly within a factor of two across the graphs; this approach even scales!</p>

<p><b>Note</b>: You might notice that the numbers for the smaller graphs are not the same as in the previous post. Because the common crawl dataset is so large, I was only able to process it in Hilbert curve order. It seemed appropriate to use the corresponding numbers for the smaller graphs, rather than the vertex order numbers.</p>

<h3 id="a-cost-challenge-for-scalable-systems">A COST challenge for scalable systems</h3>

<p>I’d like to challenge all existing graph processing platforms to evaluate and report the COST (Configuration that Outperforms a Single Thread) for this dataset. It is nearly two orders of magnitude larger than what most systems currently use for their evaluation, and nearly one order of magnitude <em>smaller</em> than can be easily processed by my laptop. It is available for everyone to download, so get to it!</p>

<p>The only data we have is for Giraph, for which 200 machines is more than enough.
I don’t think we know very much about any other scalable graph processing system. Let’s fix that!</p>

<p>I’ll happily post updates for performance numbers on specific systems. To make it fun, let’s say that each submission gets to name another system whose row sits at “NA” until its measurements arrive.</p>

<h2 id="implementation-notes">Implementation notes</h2>

<p>There were some relatively minor implementation details involved in getting 128 billion edges onto my laptop, and then processed efficiently (which isn’t to say that I got them right the first time). Doing the math, if one writes down a <code>(u32, u32)</code> for each edge, this ends up at a terabyte of data, and my laptop doesn’t have that kind of SSD. So, we’ll have to be a bit smarter.</p>

<p>Using the Hilbert curve layout leads to a very simple compressed representation: having sorted the <code>u64</code> values it uses to represent edges, one can simply delta-encode them (writing the differences between adjacent values). Because the Hilbert curve teases out locality, the gaps are often quite small, and therefore compress nicely. When I combined a variable-length integer encoding with gzip, the resulting file was onle 45GB; about 2.8 bits per edge). Decompressed (with just the variable-length delta encoding), it is 154GB, which is the representation I used to get the measurements above.</p>

<p>Not only is the Hilbert curve good for compression, it is pretty much mandatory due to the locality it provides when accessing the per-vertex state. Random access into main memory is not all that bad, but when the working set for vertex state in pagerank is 40GB, random access turns in to thrashing. This is not only slow, but it wears out my SSD, and no one wants that. The near-linear performance scaling as we went up two orders of magnitude in size is evidence for how cool the Hilbert curve is.</p>

<p>Transforming the data to the compressed Hilbert curve encoding was relatively efficient. My laptop was able to transform (encode, sort, and compress) the data faster than I could download it, so I didn’t work too hard to optimize the performance. However, this is an excellent example of where parallelism can help: each of the <a href="http://webdatacommons.org/hyperlinkgraph/2012-08/data/arc.list.txt">700 files comprising the graph</a> could be processed independently (and I did fire up a second process on occasion). If you had 100 cores (like most of the scalable systems use) the whole conversion process would just take a few minutes.</p>

<p>The implementation is available in the <a href="https://github.com/frankmcsherry/COST">COST repository</a>, though the documentation is not yet brilliant. If you have questions, especially if you’d like to be doing this sort of thing in your scalable system and want pointers, just shoot me an email.</p>

<h2 id="whats-next">What’s next</h2>

<p>I’m not going to try this nonsense on any larger graphs. The ball is now in the court of the scalable systems. We’ll just have to wait and see how they do before setting any stretch performance goals.</p>

<p>Instead, it’s time to get back to work on timely dataflow in Rust! Stay tuned for that kind of post.</p>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Frank McSherry</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Frank McSherry</li>
          <li><a href="mailto:mcsherry@gmail.com">mcsherry@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/frankmcsherry">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">frankmcsherry</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/frankmcsherry">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">frankmcsherry</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">Some notes on doing less of what I used to do, and more of what I'd rather be doing.</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
